{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "international-moses",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sagemaker\n",
    "import argparse\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "exposed-fireplace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (1.3.0)\n",
      "Collecting datasets\n",
      "  Downloading datasets-1.5.0-py3-none-any.whl (192 kB)\n",
      "\u001b[K     |████████████████████████████████| 192 kB 17.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: xxhash in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from datasets) (2.0.0)\n",
      "Requirement already satisfied: tqdm<4.50.0,>=4.27 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from datasets) (4.42.1)\n",
      "Requirement already satisfied: dataclasses in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from datasets) (0.8)\n",
      "Requirement already satisfied: importlib-metadata in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from datasets) (3.4.0)\n",
      "Requirement already satisfied: fsspec in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from datasets) (0.6.2)\n",
      "Requirement already satisfied: huggingface-hub<0.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from datasets) (0.0.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from datasets) (2.25.1)\n",
      "Requirement already satisfied: multiprocess in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from datasets) (0.70.11.1)\n",
      "Requirement already satisfied: pyarrow>=0.17.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from datasets) (2.0.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from datasets) (1.19.5)\n",
      "Requirement already satisfied: dill in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from datasets) (0.3.3)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from datasets) (1.0.1)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from huggingface-hub<0.1.0->datasets) (3.0.12)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests>=2.19.0->datasets) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests>=2.19.0->datasets) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests>=2.19.0->datasets) (1.26.2)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests>=2.19.0->datasets) (4.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from importlib-metadata->datasets) (3.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from importlib-metadata->datasets) (3.7.4.3)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pandas->datasets) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pandas->datasets) (2019.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from python-dateutil>=2.6.1->pandas->datasets) (1.15.0)\n",
      "Installing collected packages: datasets\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 1.3.0\n",
      "    Uninstalling datasets-1.3.0:\n",
      "      Successfully uninstalled datasets-1.3.0\n",
      "Successfully installed datasets-1.5.0\n",
      "\u001b[33mWARNING: You are using pip version 20.3.3; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install --upgrade datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nuclear-dollar",
   "metadata": {},
   "source": [
    "# Download Adverse Drug Reaction data from HuggingFace "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "skilled-flour",
   "metadata": {},
   "source": [
    "https://huggingface.co/datasets/ade_corpus_v2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "hindu-rebound",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset ade_corpus_v2 (/home/ec2-user/.cache/huggingface/datasets/ade_corpus_v2/Ade_corpus_v2_classification/1.0.0/df238bf01b826b881a7cfc4a778a953409c9c4075eb3e4274e7a83f74379ab32)\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"ade_corpus_v2\", \"Ade_corpus_v2_classification\")\n",
    "df_context, df_label = dataset['train'].__getitem__('text'), dataset['train'].__getitem__('label')\n",
    "df = pd.DataFrame(\n",
    "    {'text': df_context,\n",
    "     'label': df_label\n",
    "    })\n",
    "df = df.rename(columns={'text':'CONTENT', 'label':'WORKFLOW_CLASSIFICATION'})\n",
    "df['WORKFLOW_CLASSIFICATION'] = df['WORKFLOW_CLASSIFICATION'].apply(lambda x: 'Adverse Event (AE)' if x == 1 else 'non ae')\n",
    "\n",
    "# Create train and validation datasets\n",
    "train, valid = train_test_split(df, test_size=0.20,shuffle = True, random_state = 2678,  stratify=df[['WORKFLOW_CLASSIFICATION']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "parallel-workshop",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save train and valid datasets\n",
    "train.to_csv(\"./data/train.csv\")\n",
    "valid.to_csv(\"./data/valid.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interpreted-height",
   "metadata": {},
   "source": [
    "# Process Raw data and load it to S3 for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "tribal-apollo",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_dir):\n",
    "    \n",
    "    df = pd.read_csv(data_dir)\n",
    "    label2id = {'Adverse Event (AE)': 1, 'non ae': 0}\n",
    "    df[\"label\"] = df[\"WORKFLOW_CLASSIFICATION\"].map(lambda x: label2id[x])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "oriental-cable",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = load_data(\"./data/train.csv\")\n",
    "df_valid = load_data(\"./data/valid.csv\")\n",
    "\n",
    "local_data_dir = \"./data/data_model\"\n",
    "os.mkdir(local_data_dir)\n",
    "\n",
    "df_train.to_csv(os.path.join(local_data_dir, \"train.csv\"), index=False)\n",
    "df_valid.to_csv(os.path.join(local_data_dir, \"valid.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "completed-bruce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18812, 4704)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train), len(df_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "destroyed-declaration",
   "metadata": {},
   "source": [
    "## Upload to S3 for SageMaker model training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "universal-allocation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input spec (in this case, just an S3 path): s3://sagemaker-us-east-1-649363377072/HF_models/AE_bert/data\n"
     ]
    }
   ],
   "source": [
    "task_name = 'AE_bert/data'\n",
    "s3_prefix = 'HF_models/' + task_name\n",
    "\n",
    "# data path in SageMaker notebook instance.\n",
    "data_dir = local_data_dir\n",
    "\n",
    "# upload data to S3\n",
    "inputs_data = sagemaker_session.upload_data(path=data_dir, bucket=bucket, key_prefix=s3_prefix)\n",
    "print('input spec (in this case, just an S3 path): {}'.format(inputs_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unable-train",
   "metadata": {},
   "source": [
    "# SageMaker model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "burning-aquarium",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "vocal-bottom",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters={'epochs': 4,\n",
    "                 'train_batch_size': 64,\n",
    "                 'max_seq_length': 128,\n",
    "                 'learning_rate': 5e-5,\n",
    "                 'model_name':'distilbert-base-uncased',\n",
    "                 'text_column':'CONTENT',\n",
    "                 'label_column': 'label'\n",
    "                 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saved-benchmark",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "wanted-encyclopedia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amazon SageMaker PyTorch framework\n",
    "train_instance_type = 'ml.p3.2xlarge'\n",
    "\n",
    "bert_estimator = PyTorch(entry_point='hf_train_deploy.py',\n",
    "                    source_dir = 'src',\n",
    "                    role=role,\n",
    "                    framework_version='1.4.0',\n",
    "                    py_version='py3',\n",
    "                    instance_count=1,\n",
    "                    instance_type= train_instance_type,#'local',\n",
    "                    hyperparameters = hyperparameters\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "incredible-clerk",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-08 14:56:50 Starting - Starting the training job...ProfilerReport-1615215410: InProgress\n",
      "...\n",
      "2021-03-08 14:57:49 Starting - Launching requested ML instances.........\n",
      "2021-03-08 14:59:10 Starting - Preparing the instances for training......\n",
      "2021-03-08 15:00:20 Downloading - Downloading input data...\n",
      "2021-03-08 15:00:51 Training - Downloading the training image.........\n",
      "2021-03-08 15:02:12 Training - Training image download completed. Training in progress.\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2021-03-08 15:02:08,237 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2021-03-08 15:02:08,260 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2021-03-08 15:02:11,275 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2021-03-08 15:02:11,592 sagemaker-containers INFO     Module default_user_module_name does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2021-03-08 15:02:11,592 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2021-03-08 15:02:11,592 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2021-03-08 15:02:11,592 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python -m pip install . -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing /tmp/tmp9teojbgf/module_dir\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 1)) (4.42.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests==2.22.0 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 2)) (2.22.0)\u001b[0m\n",
      "\u001b[34mCollecting regex\n",
      "  Downloading regex-2020.11.13-cp36-cp36m-manylinux2014_x86_64.whl (723 kB)\u001b[0m\n",
      "\u001b[34mCollecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.95-cp36-cp36m-manylinux2014_x86_64.whl (1.2 MB)\u001b[0m\n",
      "\u001b[34mCollecting sacremoses\n",
      "  Downloading sacremoses-0.0.43.tar.gz (883 kB)\u001b[0m\n",
      "\u001b[34mCollecting transformers\n",
      "  Downloading transformers-4.3.3-py3-none-any.whl (1.9 MB)\u001b[0m\n",
      "\u001b[34mCollecting datasets\n",
      "  Downloading datasets-1.4.1-py3-none-any.whl (186 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests==2.22.0->-r requirements.txt (line 2)) (1.25.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests==2.22.0->-r requirements.txt (line 2)) (2.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests==2.22.0->-r requirements.txt (line 2)) (3.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests==2.22.0->-r requirements.txt (line 2)) (2020.4.5.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from sacremoses->-r requirements.txt (line 5)) (1.14.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from sacremoses->-r requirements.txt (line 5)) (7.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from sacremoses->-r requirements.txt (line 5)) (0.15.1)\u001b[0m\n",
      "\u001b[34mCollecting tokenizers<0.11,>=0.10.1\n",
      "  Downloading tokenizers-0.10.1-cp36-cp36m-manylinux2010_x86_64.whl (3.2 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.6/site-packages (from transformers->-r requirements.txt (line 6)) (20.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /opt/conda/lib/python3.6/site-packages (from transformers->-r requirements.txt (line 6)) (1.6.0)\u001b[0m\n",
      "\u001b[34mCollecting numpy>=1.17\n",
      "  Downloading numpy-1.19.5-cp36-cp36m-manylinux2010_x86_64.whl (14.8 MB)\u001b[0m\n",
      "\u001b[34mCollecting filelock\n",
      "  Downloading filelock-3.0.12-py3-none-any.whl (7.6 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dataclasses; python_version < \"3.7\" in /opt/conda/lib/python3.6/site-packages (from transformers->-r requirements.txt (line 6)) (0.7)\u001b[0m\n",
      "\u001b[34mCollecting multiprocess\n",
      "  Downloading multiprocess-0.70.11.1-py36-none-any.whl (101 kB)\u001b[0m\n",
      "\u001b[34mCollecting dill\n",
      "  Downloading dill-0.3.3-py2.py3-none-any.whl (81 kB)\u001b[0m\n",
      "\u001b[34mCollecting fsspec\n",
      "  Downloading fsspec-0.8.7-py3-none-any.whl (103 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.6/site-packages (from datasets->-r requirements.txt (line 7)) (0.25.0)\u001b[0m\n",
      "\u001b[34mCollecting pyarrow>=0.17.1\n",
      "  Downloading pyarrow-3.0.0-cp36-cp36m-manylinux2014_x86_64.whl (20.7 MB)\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub==0.0.2\n",
      "  Downloading huggingface_hub-0.0.2-py3-none-any.whl (24 kB)\u001b[0m\n",
      "\u001b[34mCollecting xxhash\n",
      "  Downloading xxhash-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (242 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging->transformers->-r requirements.txt (line 6)) (2.4.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata; python_version < \"3.8\"->transformers->-r requirements.txt (line 6)) (3.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.6/site-packages (from pandas->datasets->-r requirements.txt (line 7)) (2.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.6/site-packages (from pandas->datasets->-r requirements.txt (line 7)) (2020.1)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: sacremoses, default-user-module-name\n",
      "  Building wheel for sacremoses (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for sacremoses (setup.py): finished with status 'done'\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.43-py3-none-any.whl size=893259 sha256=867cd8e5962f35468ffd5bd095fa743ec90dba7e0e5d8eaf93bd8ea5ceb393aa\n",
      "  Stored in directory: /root/.cache/pip/wheels/49/25/98/cdea9c79b2d9a22ccc59540b1784b67f06b633378e97f58da2\n",
      "  Building wheel for default-user-module-name (setup.py): started\n",
      "  Building wheel for default-user-module-name (setup.py): finished with status 'done'\n",
      "  Created wheel for default-user-module-name: filename=default_user_module_name-1.0.0-py2.py3-none-any.whl size=10764 sha256=ac920351aa14253784bfa07aa5c60b5da798b219d0673241e17e3888bbd3b005\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-p_c3wrb4/wheels/23/df/bb/bcbcb0f6ef29cfb9639bd9b28c75cbd3cd0c2d9ec2c37dcaa5\u001b[0m\n",
      "\u001b[34mSuccessfully built sacremoses default-user-module-name\u001b[0m\n",
      "\u001b[34mInstalling collected packages: regex, sentencepiece, sacremoses, tokenizers, numpy, filelock, transformers, dill, multiprocess, fsspec, pyarrow, huggingface-hub, xxhash, datasets, default-user-module-name\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.16.4\n",
      "    Uninstalling numpy-1.16.4:\n",
      "      Successfully uninstalled numpy-1.16.4\u001b[0m\n",
      "\u001b[34mSuccessfully installed datasets-1.4.1 default-user-module-name-1.0.0 dill-0.3.3 filelock-3.0.12 fsspec-0.8.7 huggingface-hub-0.0.2 multiprocess-0.70.11.1 numpy-1.19.5 pyarrow-3.0.0 regex-2020.11.13 sacremoses-0.0.43 sentencepiece-0.1.95 tokenizers-0.10.1 transformers-4.3.3 xxhash-2.0.0\u001b[0m\n",
      "\u001b[34mWARNING: You are using pip version 20.1.1; however, version 21.0.1 is available.\u001b[0m\n",
      "\u001b[34mYou should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[34m2021-03-08 15:02:26,170 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"train_batch_size\": 64,\n",
      "        \"model_name\": \"distilbert-base-uncased\",\n",
      "        \"label_column\": \"label\",\n",
      "        \"max_seq_length\": 128,\n",
      "        \"epochs\": 4,\n",
      "        \"learning_rate\": 5e-05,\n",
      "        \"text_column\": \"CONTENT\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"pytorch-training-2021-03-08-14-56-50-390\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-649363377072/pytorch-training-2021-03-08-14-56-50-390/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"hf_train_deploy\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"hf_train_deploy.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":4,\"label_column\":\"label\",\"learning_rate\":5e-05,\"max_seq_length\":128,\"model_name\":\"distilbert-base-uncased\",\"text_column\":\"CONTENT\",\"train_batch_size\":64}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=hf_train_deploy.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=hf_train_deploy\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-649363377072/pytorch-training-2021-03-08-14-56-50-390/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":4,\"label_column\":\"label\",\"learning_rate\":5e-05,\"max_seq_length\":128,\"model_name\":\"distilbert-base-uncased\",\"text_column\":\"CONTENT\",\"train_batch_size\":64},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-training-2021-03-08-14-56-50-390\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-649363377072/pytorch-training-2021-03-08-14-56-50-390/source/sourcedir.tar.gz\",\"module_name\":\"hf_train_deploy\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"hf_train_deploy.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"4\",\"--label_column\",\"label\",\"--learning_rate\",\"5e-05\",\"--max_seq_length\",\"128\",\"--model_name\",\"distilbert-base-uncased\",\"--text_column\",\"CONTENT\",\"--train_batch_size\",\"64\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_BATCH_SIZE=64\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_NAME=distilbert-base-uncased\u001b[0m\n",
      "\u001b[34mSM_HP_LABEL_COLUMN=label\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_SEQ_LENGTH=128\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=4\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=5e-05\u001b[0m\n",
      "\u001b[34mSM_HP_TEXT_COLUMN=CONTENT\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python hf_train_deploy.py --epochs 4 --label_column label --learning_rate 5e-05 --max_seq_length 128 --model_name distilbert-base-uncased --text_column CONTENT --train_batch_size 64\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34m2021-03-08 15:02:30,363 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): huggingface.co:443\u001b[0m\n",
      "\u001b[34m2021-03-08 15:02:30,459 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 \"HEAD /distilbert-base-uncased/resolve/main/config.json HTTP/1.1\" 200 0\u001b[0m\n",
      "\u001b[34m2021-03-08 15:02:30,460 - filelock - DEBUG - Attempting to acquire lock 140556705795600 on /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.d423bdf2f58dc8b77d5f5d18028d7ae4a72dcfd8f468e81fe979ada957a8c361.lock\u001b[0m\n",
      "\u001b[34m2021-03-08 15:02:30,461 - filelock - INFO - Lock 140556705795600 acquired on /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.d423bdf2f58dc8b77d5f5d18028d7ae4a72dcfd8f468e81fe979ada957a8c361.lock\u001b[0m\n",
      "\u001b[34m2021-03-08 15:02:30,463 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): huggingface.co:443\u001b[0m\n",
      "\u001b[34m2021-03-08 15:02:30,477 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 \"GET /distilbert-base-uncased/resolve/main/config.json HTTP/1.1\" 200 442\u001b[0m\n",
      "\u001b[34m2021-03-08 15:02:30,479 - filelock - DEBUG - Attempting to release lock 140556705795600 on /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.d423bdf2f58dc8b77d5f5d18028d7ae4a72dcfd8f468e81fe979ada957a8c361.lock\u001b[0m\n",
      "\u001b[34m2021-03-08 15:02:30,479 - filelock - INFO - Lock 140556705795600 released on /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.d423bdf2f58dc8b77d5f5d18028d7ae4a72dcfd8f468e81fe979ada957a8c361.lock\u001b[0m\n",
      "\u001b[34m2021-03-08 15:02:30,482 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): huggingface.co:443\u001b[0m\n",
      "\u001b[34m2021-03-08 15:02:30,498 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 \"HEAD /bert-base-uncased/resolve/main/vocab.txt HTTP/1.1\" 200 0\u001b[0m\n",
      "\u001b[34m2021-03-08 15:02:30,499 - filelock - DEBUG - Attempting to acquire lock 140556705458440 on /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock\u001b[0m\n",
      "\u001b[34m2021-03-08 15:02:30,500 - filelock - INFO - Lock 140556705458440 acquired on /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock\u001b[0m\n",
      "\u001b[34m2021-03-08 15:02:30,501 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): huggingface.co:443\u001b[0m\n",
      "\u001b[34m2021-03-08 15:02:30,519 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 \"GET /bert-base-uncased/resolve/main/vocab.txt HTTP/1.1\" 200 231508\u001b[0m\n",
      "\u001b[34m2021-03-08 15:02:30,526 - filelock - DEBUG - Attempting to release lock 140556705458440 on /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock\u001b[0m\n",
      "\u001b[34m2021-03-08 15:02:30,526 - filelock - INFO - Lock 140556705458440 released on /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock\u001b[0m\n",
      "\u001b[34m2021-03-08 15:02:30,528 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): huggingface.co:443\u001b[0m\n",
      "\u001b[34m2021-03-08 15:02:30,545 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 \"HEAD /bert-base-uncased/resolve/main/tokenizer.json HTTP/1.1\" 200 0\u001b[0m\n",
      "\u001b[34m2021-03-08 15:02:30,546 - filelock - DEBUG - Attempting to acquire lock 140556705456256 on /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4.lock\u001b[0m\n",
      "\u001b[34m2021-03-08 15:02:30,547 - filelock - INFO - Lock 140556705456256 acquired on /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4.lock\u001b[0m\n",
      "\u001b[34m2021-03-08 15:02:30,548 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): huggingface.co:443\u001b[0m\n",
      "\u001b[34m2021-03-08 15:02:30,568 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 \"GET /bert-base-uncased/resolve/main/tokenizer.json HTTP/1.1\" 200 466062\u001b[0m\n",
      "\u001b[34m2021-03-08 15:02:30,581 - filelock - DEBUG - Attempting to release lock 140556705456256 on /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4.lock\u001b[0m\n",
      "\u001b[34m2021-03-08 15:02:30,581 - filelock - INFO - Lock 140556705456256 released on /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4.lock\u001b[0m\n",
      "\u001b[34m2021-03-08 15:02:30,762 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): s3.amazonaws.com:443\u001b[0m\n",
      "\u001b[34m2021-03-08 15:02:30,799 - urllib3.connectionpool - DEBUG - https://s3.amazonaws.com:443 \"HEAD /datasets.huggingface.co/datasets/datasets/csv/csv.py HTTP/1.1\" 200 0\u001b[0m\n",
      "\u001b[34m2021-03-08 15:02:30,801 - datasets.builder - WARNING - Using custom data configuration default-b97809f51dc2e263\u001b[0m\n",
      "\u001b[34mDownloading and preparing dataset csv/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/csv/default-b97809f51dc2e263/0.0.0/2a88c45fed596f9421a2e7f74ab1a3cd012ef75210a5dc1950e8d60ca8d9c66c...\u001b[0m\n",
      "\u001b[34mDataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-b97809f51dc2e263/0.0.0/2a88c45fed596f9421a2e7f74ab1a3cd012ef75210a5dc1950e8d60ca8d9c66c. Subsequent calls will reuse this data.\u001b[0m\n",
      "\u001b[34m2021-03-08 15:02:30,883 - datasets.utils.deprecation_utils - WARNING - rename_column_ is deprecated and will be removed in the next major version of datasets. Please use :func:`DatasetDict.rename_column` instead.\u001b[0m\n",
      "\u001b[34m2021-03-08 15:02:45,616 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): s3.amazonaws.com:443\u001b[0m\n",
      "\u001b[34m2021-03-08 15:02:45,647 - urllib3.connectionpool - DEBUG - https://s3.amazonaws.com:443 \"HEAD /datasets.huggingface.co/datasets/datasets/csv/csv.py HTTP/1.1\" 200 0\u001b[0m\n",
      "\u001b[34m2021-03-08 15:02:45,650 - datasets.builder - WARNING - Using custom data configuration default-fa8e4d08e03ba608\u001b[0m\n",
      "\u001b[34mDownloading and preparing dataset csv/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/csv/default-fa8e4d08e03ba608/0.0.0/2a88c45fed596f9421a2e7f74ab1a3cd012ef75210a5dc1950e8d60ca8d9c66c...\u001b[0m\n",
      "\u001b[34mDataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-fa8e4d08e03ba608/0.0.0/2a88c45fed596f9421a2e7f74ab1a3cd012ef75210a5dc1950e8d60ca8d9c66c. Subsequent calls will reuse this data.\u001b[0m\n",
      "\u001b[34m2021-03-08 15:02:49,680 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): huggingface.co:443\u001b[0m\n",
      "\u001b[34m2021-03-08 15:02:49,694 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 \"HEAD /distilbert-base-uncased/resolve/main/config.json HTTP/1.1\" 200 0\u001b[0m\n",
      "\u001b[34m2021-03-08 15:02:49,699 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): huggingface.co:443\u001b[0m\n",
      "\u001b[34m2021-03-08 15:02:49,713 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 \"HEAD /distilbert-base-uncased/resolve/main/pytorch_model.bin HTTP/1.1\" 302 0\u001b[0m\n",
      "\u001b[34m2021-03-08 15:02:49,717 - filelock - DEBUG - Attempting to acquire lock 140556564510536 on /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a.lock\u001b[0m\n",
      "\u001b[34m2021-03-08 15:02:49,718 - filelock - INFO - Lock 140556564510536 acquired on /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a.lock\u001b[0m\n",
      "\u001b[34m2021-03-08 15:02:49,720 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): cdn-lfs.huggingface.co:443\u001b[0m\n",
      "\u001b[34m2021-03-08 15:02:49,842 - urllib3.connectionpool - DEBUG - https://cdn-lfs.huggingface.co:443 \"GET /distilbert-base-uncased/e60d71610916da4787c5513c81bc026d415708528295502fb3e1a6fe1485ea7c HTTP/1.1\" 200 267967963\u001b[0m\n",
      "\u001b[34m2021-03-08 15:02:54,586 - filelock - DEBUG - Attempting to release lock 140556564510536 on /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a.lock\u001b[0m\n",
      "\u001b[34m2021-03-08 15:02:54,586 - filelock - INFO - Lock 140556564510536 released on /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a.lock\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:02:59.264 algo-1:52 INFO json_config.py:90] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:02:59.264 algo-1:52 INFO hook.py:191] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:02:59.264 algo-1:52 INFO hook.py:236] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:02:59.264 algo-1:52 INFO state_store.py:67] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:02:59.287 algo-1:52 INFO hook.py:376] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:02:59.287 algo-1:52 INFO hook.py:437] Hook is writing from the hook with pid: 52\n",
      "\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:03:00.114 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:03:00.114 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:03:00.125 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:DistilBertForSequenceClassification SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m2021-03-08 15:03:00,294 - root - DEBUG - Writing metric: _RawMetricData(MetricName='CrossEntropyLoss_output_0_GLOBAL',Value=0.6517201066017151,Timestamp=1615215780.1254563,IterationNumber=0)\u001b[0m\n",
      "\u001b[34m2021-03-08 15:03:00,295 - root - ERROR - 'NoneType' object has no attribute 'write'\u001b[0m\n",
      "\u001b[34m{'loss': 0.4961, 'learning_rate': 4.9573924158500216e-05, 'epoch': 0.04}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.36155784130096436, 'eval_accuracy': 0.8339710884353742, 'eval_f1': 0.7448546226723294, 'eval_precision': 0.6717737183264585, 'eval_recall': 0.8357771260997068, 'eval_runtime': 15.1534, 'eval_samples_per_second': 310.426, 'epoch': 0.04}\u001b[0m\n",
      "\u001b[34m{'loss': 0.3189, 'learning_rate': 4.904132935662548e-05, 'epoch': 0.09}\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:03:34.628 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:03:34.628 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:03:34.629 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:DistilBertForSequenceClassification SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.26723912358283997, 'eval_accuracy': 0.8869047619047619, 'eval_f1': 0.8120141342756184, 'eval_precision': 0.7837653478854024, 'eval_recall': 0.842375366568915, 'eval_runtime': 15.139, 'eval_samples_per_second': 310.721, 'epoch': 0.09}\u001b[0m\n",
      "\u001b[34m{'loss': 0.3152, 'learning_rate': 4.850873455475074e-05, 'epoch': 0.13}\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:04:05.237 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:04:05.237 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:04:05.238 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:DistilBertForSequenceClassification SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.28245800733566284, 'eval_accuracy': 0.8881802721088435, 'eval_f1': 0.7786195286195285, 'eval_precision': 0.9140316205533597, 'eval_recall': 0.6781524926686217, 'eval_runtime': 15.226, 'eval_samples_per_second': 308.946, 'epoch': 0.13}\u001b[0m\n",
      "\u001b[34m{'loss': 0.3272, 'learning_rate': 4.7976139752876016e-05, 'epoch': 0.17}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.2568991780281067, 'eval_accuracy': 0.905187074829932, 'eval_f1': 0.8265940902021773, 'eval_precision': 0.8799668874172185, 'eval_recall': 0.7793255131964809, 'eval_runtime': 15.1218, 'eval_samples_per_second': 311.073, 'epoch': 0.17}\u001b[0m\n",
      "\u001b[34m{'loss': 0.2778, 'learning_rate': 4.7443544951001276e-05, 'epoch': 0.21}\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:04:39.659 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:04:39.659 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:04:39.660 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:DistilBertForSequenceClassification SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.25771215558052063, 'eval_accuracy': 0.8854166666666666, 'eval_f1': 0.8228721656260269, 'eval_precision': 0.7456819535437761, 'eval_recall': 0.9178885630498533, 'eval_runtime': 14.9941, 'eval_samples_per_second': 313.722, 'epoch': 0.21}\u001b[0m\n",
      "\u001b[34m{'loss': 0.2639, 'learning_rate': 4.691095014912655e-05, 'epoch': 0.26}\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:05:10.093 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:05:10.093 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:05:10.093 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:DistilBertForSequenceClassification SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.20928147435188293, 'eval_accuracy': 0.9166666666666666, 'eval_f1': 0.8568298027757487, 'eval_precision': 0.8537117903930131, 'eval_recall': 0.8599706744868035, 'eval_runtime': 15.3142, 'eval_samples_per_second': 307.165, 'epoch': 0.26}\u001b[0m\n",
      "\u001b[34m{'loss': 0.2218, 'learning_rate': 4.637835534725181e-05, 'epoch': 0.3}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.22335724532604218, 'eval_accuracy': 0.9221938775510204, 'eval_f1': 0.8582494190549962, 'eval_precision': 0.909688013136289, 'eval_recall': 0.8123167155425219, 'eval_runtime': 15.0545, 'eval_samples_per_second': 312.464, 'epoch': 0.3}\u001b[0m\n",
      "\u001b[34m{'loss': 0.2392, 'learning_rate': 4.584576054537708e-05, 'epoch': 0.34}\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:05:44.674 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:05:44.674 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:05:44.675 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:DistilBertForSequenceClassification SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.19971609115600586, 'eval_accuracy': 0.920280612244898, 'eval_f1': 0.8708232862555976, 'eval_precision': 0.8213125406107862, 'eval_recall': 0.9266862170087976, 'eval_runtime': 15.2941, 'eval_samples_per_second': 307.57, 'epoch': 0.34}\u001b[0m\n",
      "\u001b[34m{'loss': 0.257, 'learning_rate': 4.531316574350234e-05, 'epoch': 0.38}\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:06:14.980 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:06:14.980 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:06:14.981 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:DistilBertForSequenceClassification SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.19789642095565796, 'eval_accuracy': 0.9253826530612245, 'eval_f1': 0.8664891593761888, 'eval_precision': 0.9003952569169961, 'eval_recall': 0.8350439882697948, 'eval_runtime': 14.9983, 'eval_samples_per_second': 313.635, 'epoch': 0.38}\u001b[0m\n",
      "\u001b[34m{'loss': 0.2511, 'learning_rate': 4.4780570941627616e-05, 'epoch': 0.43}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.22125521302223206, 'eval_accuracy': 0.9168792517006803, 'eval_f1': 0.8435374149659863, 'eval_precision': 0.9286343612334802, 'eval_recall': 0.7727272727272727, 'eval_runtime': 15.0266, 'eval_samples_per_second': 313.045, 'epoch': 0.43}\u001b[0m\n",
      "\u001b[34m{'loss': 0.2053, 'learning_rate': 4.4247976139752876e-05, 'epoch': 0.47}\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:06:50.743 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:06:50.743 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:06:50.743 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:DistilBertForSequenceClassification SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.1820468008518219, 'eval_accuracy': 0.9302721088435374, 'eval_f1': 0.8748091603053435, 'eval_precision': 0.9124203821656051, 'eval_recall': 0.8401759530791789, 'eval_runtime': 15.1561, 'eval_samples_per_second': 310.371, 'epoch': 0.47}\u001b[0m\n",
      "\u001b[34m{'loss': 0.2139, 'learning_rate': 4.371538133787814e-05, 'epoch': 0.51}\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:07:20.978 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:07:20.978 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:07:20.979 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:DistilBertForSequenceClassification SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.17224344611167908, 'eval_accuracy': 0.9334608843537415, 'eval_f1': 0.8844592100406055, 'eval_precision': 0.8907063197026023, 'eval_recall': 0.8782991202346041, 'eval_runtime': 15.4763, 'eval_samples_per_second': 303.948, 'epoch': 0.51}\u001b[0m\n",
      "\u001b[34m{'loss': 0.2417, 'learning_rate': 4.318278653600341e-05, 'epoch': 0.55}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.17858710885047913, 'eval_accuracy': 0.9360119047619048, 'eval_f1': 0.8870544090056285, 'eval_precision': 0.9085318985395849, 'eval_recall': 0.8665689149560117, 'eval_runtime': 15.0397, 'eval_samples_per_second': 312.772, 'epoch': 0.55}\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:07:54.249 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:07:54.249 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:07:54.250 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:DistilBertForSequenceClassification SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m{'loss': 0.2114, 'learning_rate': 4.2650191734128676e-05, 'epoch': 0.6}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.17617326974868774, 'eval_accuracy': 0.9336734693877551, 'eval_f1': 0.8785992217898833, 'eval_precision': 0.9361525704809287, 'eval_recall': 0.8277126099706745, 'eval_runtime': 15.2344, 'eval_samples_per_second': 308.774, 'epoch': 0.6}\u001b[0m\n",
      "\u001b[34m{'loss': 0.2512, 'learning_rate': 4.211759693225394e-05, 'epoch': 0.64}\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:08:25.896 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:08:25.896 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:08:25.897 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:DistilBertForSequenceClassification SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.15731053054332733, 'eval_accuracy': 0.9400510204081632, 'eval_f1': 0.8967032967032967, 'eval_precision': 0.8960468521229868, 'eval_recall': 0.8973607038123167, 'eval_runtime': 15.0213, 'eval_samples_per_second': 313.155, 'epoch': 0.64}\u001b[0m\n",
      "\u001b[34m{'loss': 0.1604, 'learning_rate': 4.158500213037921e-05, 'epoch': 0.68}\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:08:56.465 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:08:56.466 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:08:56.466 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:DistilBertForSequenceClassification SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.1685652881860733, 'eval_accuracy': 0.938562925170068, 'eval_f1': 0.895554752439465, 'eval_precision': 0.8831076265146115, 'eval_recall': 0.908357771260997, 'eval_runtime': 15.1026, 'eval_samples_per_second': 311.469, 'epoch': 0.68}\u001b[0m\n",
      "\u001b[34m{'loss': 0.2111, 'learning_rate': 4.1052407328504476e-05, 'epoch': 0.72}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.1761290431022644, 'eval_accuracy': 0.9311224489795918, 'eval_f1': 0.8889650445510624, 'eval_precision': 0.8346203346203346, 'eval_recall': 0.9508797653958945, 'eval_runtime': 15.371, 'eval_samples_per_second': 306.031, 'epoch': 0.72}\u001b[0m\n",
      "\u001b[34m{'loss': 0.1956, 'learning_rate': 4.051981252662974e-05, 'epoch': 0.77}\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:09:31.186 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:09:31.186 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:09:31.187 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:DistilBertForSequenceClassification SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.17163020372390747, 'eval_accuracy': 0.9326105442176871, 'eval_f1': 0.8901213171577124, 'eval_precision': 0.8441814595660749, 'eval_recall': 0.9413489736070382, 'eval_runtime': 15.2035, 'eval_samples_per_second': 309.403, 'epoch': 0.77}\u001b[0m\n",
      "\u001b[34m{'loss': 0.2012, 'learning_rate': 3.998721772475501e-05, 'epoch': 0.81}\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:10:01.783 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:10:01.783 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:10:01.784 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:DistilBertForSequenceClassification SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.16361823678016663, 'eval_accuracy': 0.938562925170068, 'eval_f1': 0.8989157047918853, 'eval_precision': 0.8595317725752508, 'eval_recall': 0.9420821114369502, 'eval_runtime': 15.2045, 'eval_samples_per_second': 309.383, 'epoch': 0.81}\u001b[0m\n",
      "\u001b[34m{'loss': 0.1953, 'learning_rate': 3.9454622922880276e-05, 'epoch': 0.85}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.15564580261707306, 'eval_accuracy': 0.9438775510204082, 'eval_f1': 0.9053084648493545, 'eval_precision': 0.8862359550561798, 'eval_recall': 0.9252199413489736, 'eval_runtime': 15.0491, 'eval_samples_per_second': 312.577, 'epoch': 0.85}\u001b[0m\n",
      "\u001b[34m{'loss': 0.184, 'learning_rate': 3.8922028121005535e-05, 'epoch': 0.89}\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:10:37.249 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:10:37.249 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:10:37.250 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:DistilBertForSequenceClassification SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.1754688322544098, 'eval_accuracy': 0.9451530612244898, 'eval_f1': 0.904089219330855, 'eval_precision': 0.9170437405731523, 'eval_recall': 0.8914956011730205, 'eval_runtime': 14.975, 'eval_samples_per_second': 314.123, 'epoch': 0.89}\u001b[0m\n",
      "\u001b[34m{'loss': 0.1723, 'learning_rate': 3.838943331913081e-05, 'epoch': 0.94}\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:11:07.955 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:11:07.955 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:11:07.956 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:DistilBertForSequenceClassification SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.15273882448673248, 'eval_accuracy': 0.9426020408163265, 'eval_f1': 0.9059888579387186, 'eval_precision': 0.8627320954907162, 'eval_recall': 0.9538123167155426, 'eval_runtime': 15.6711, 'eval_samples_per_second': 300.171, 'epoch': 0.94}\u001b[0m\n",
      "\u001b[34m{'loss': 0.1973, 'learning_rate': 3.785683851725607e-05, 'epoch': 0.98}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.16651861369609833, 'eval_accuracy': 0.9415391156462585, 'eval_f1': 0.8943526699961583, 'eval_precision': 0.9394673123486683, 'eval_recall': 0.8533724340175953, 'eval_runtime': 15.0604, 'eval_samples_per_second': 312.341, 'epoch': 0.98}\u001b[0m\n",
      "\u001b[34m{'loss': 0.1545, 'learning_rate': 3.732424371538134e-05, 'epoch': 1.02}\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:11:42.585 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:11:42.585 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:11:42.586 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:DistilBertForSequenceClassification SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.17480242252349854, 'eval_accuracy': 0.9466411564625851, 'eval_f1': 0.9060980172091284, 'eval_precision': 0.9251336898395722, 'eval_recall': 0.8878299120234604, 'eval_runtime': 15.0078, 'eval_samples_per_second': 313.438, 'epoch': 1.02}\u001b[0m\n",
      "\u001b[34m{'loss': 0.1651, 'learning_rate': 3.67916489135066e-05, 'epoch': 1.06}\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:12:12.615 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:12:12.615 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:12:12.616 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:DistilBertForSequenceClassification SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.18026386201381683, 'eval_accuracy': 0.9402636054421769, 'eval_f1': 0.9018512050296892, 'eval_precision': 0.8612408272181454, 'eval_recall': 0.9464809384164223, 'eval_runtime': 15.3818, 'eval_samples_per_second': 305.816, 'epoch': 1.06}\u001b[0m\n",
      "\u001b[34m{'loss': 0.1058, 'learning_rate': 3.6259054111631875e-05, 'epoch': 1.11}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.1763443648815155, 'eval_accuracy': 0.9481292517006803, 'eval_f1': 0.9120403749098774, 'eval_precision': 0.8971631205673759, 'eval_recall': 0.9274193548387096, 'eval_runtime': 15.0982, 'eval_samples_per_second': 311.561, 'epoch': 1.11}\u001b[0m\n",
      "\u001b[34m{'loss': 0.1097, 'learning_rate': 3.5726459309757135e-05, 'epoch': 1.15}\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:12:47.785 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:12:47.785 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:12:47.786 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:DistilBertForSequenceClassification SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.19228814542293549, 'eval_accuracy': 0.9477040816326531, 'eval_f1': 0.9090909090909092, 'eval_precision': 0.9165424739195231, 'eval_recall': 0.9017595307917888, 'eval_runtime': 15.1938, 'eval_samples_per_second': 309.599, 'epoch': 1.15}\u001b[0m\n",
      "\u001b[34m{'loss': 0.102, 'learning_rate': 3.519386450788241e-05, 'epoch': 1.19}\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:13:17.715 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:13:17.715 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:13:17.715 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:DistilBertForSequenceClassification SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.17326821386814117, 'eval_accuracy': 0.9449404761904762, 'eval_f1': 0.9046742730953257, 'eval_precision': 0.9083518107908352, 'eval_recall': 0.9010263929618768, 'eval_runtime': 15.1223, 'eval_samples_per_second': 311.063, 'epoch': 1.19}\u001b[0m\n",
      "\u001b[34m{'loss': 0.1323, 'learning_rate': 3.466126970600767e-05, 'epoch': 1.23}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.1804165244102478, 'eval_accuracy': 0.9428146258503401, 'eval_f1': 0.8992131884600975, 'eval_precision': 0.9195402298850575, 'eval_recall': 0.8797653958944281, 'eval_runtime': 15.0977, 'eval_samples_per_second': 311.571, 'epoch': 1.23}\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:13:50.630 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:13:50.630 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:13:50.631 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:DistilBertForSequenceClassification SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m{'loss': 0.11, 'learning_rate': 3.4128674904132935e-05, 'epoch': 1.28}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.18465758860111237, 'eval_accuracy': 0.9426020408163265, 'eval_f1': 0.9039145907473308, 'eval_precision': 0.8782849239280774, 'eval_recall': 0.9310850439882697, 'eval_runtime': 15.4943, 'eval_samples_per_second': 303.595, 'epoch': 1.28}\u001b[0m\n",
      "\u001b[34m{'loss': 0.1107, 'learning_rate': 3.35960801022582e-05, 'epoch': 1.32}\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:14:24.104 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:14:24.104 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:14:24.105 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:DistilBertForSequenceClassification SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.2208137959241867, 'eval_accuracy': 0.9377125850340136, 'eval_f1': 0.8990699276610403, 'eval_precision': 0.847953216374269, 'eval_recall': 0.9567448680351907, 'eval_runtime': 15.4036, 'eval_samples_per_second': 305.382, 'epoch': 1.32}\u001b[0m\n",
      "\u001b[34m{'loss': 0.0915, 'learning_rate': 3.306348530038347e-05, 'epoch': 1.36}\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:14:54.971 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:14:54.971 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:14:54.972 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:DistilBertForSequenceClassification SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.205616757273674, 'eval_accuracy': 0.9494047619047619, 'eval_f1': 0.9125642909625276, 'eval_precision': 0.914580265095729, 'eval_recall': 0.9105571847507331, 'eval_runtime': 15.135, 'eval_samples_per_second': 310.803, 'epoch': 1.36}\u001b[0m\n",
      "\u001b[34m{'loss': 0.1152, 'learning_rate': 3.2530890498508735e-05, 'epoch': 1.4}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.22258372604846954, 'eval_accuracy': 0.9438775510204082, 'eval_f1': 0.9010494752623688, 'eval_precision': 0.9217791411042945, 'eval_recall': 0.8812316715542522, 'eval_runtime': 15.355, 'eval_samples_per_second': 306.35, 'epoch': 1.4}\u001b[0m\n",
      "\u001b[34m{'loss': 0.1224, 'learning_rate': 3.1998295696634e-05, 'epoch': 1.45}\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:15:29.642 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:15:29.642 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:15:29.643 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:DistilBertForSequenceClassification SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.1745089888572693, 'eval_accuracy': 0.9494047619047619, 'eval_f1': 0.9104589917231002, 'eval_precision': 0.9350850077279753, 'eval_recall': 0.8870967741935484, 'eval_runtime': 15.1525, 'eval_samples_per_second': 310.443, 'epoch': 1.45}\u001b[0m\n",
      "\u001b[34m{'loss': 0.127, 'learning_rate': 3.146570089475927e-05, 'epoch': 1.49}\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:16:00.044 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:16:00.044 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:16:00.045 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:DistilBertForSequenceClassification SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.21261803805828094, 'eval_accuracy': 0.9500425170068028, 'eval_f1': 0.911487758945386, 'eval_precision': 0.9372579395817195, 'eval_recall': 0.8870967741935484, 'eval_runtime': 15.1438, 'eval_samples_per_second': 310.623, 'epoch': 1.49}\u001b[0m\n",
      "\u001b[34m{'loss': 0.1331, 'learning_rate': 3.0933106092884535e-05, 'epoch': 1.53}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.16139784455299377, 'eval_accuracy': 0.952593537414966, 'eval_f1': 0.9175600739371534, 'eval_precision': 0.9254287844891872, 'eval_recall': 0.9098240469208211, 'eval_runtime': 15.0183, 'eval_samples_per_second': 313.218, 'epoch': 1.53}\u001b[0m\n",
      "\u001b[34m{'loss': 0.1211, 'learning_rate': 3.0400511291009802e-05, 'epoch': 1.57}\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:16:34.803 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:16:34.803 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:16:34.804 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:DistilBertForSequenceClassification SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.21612648665905, 'eval_accuracy': 0.9328231292517006, 'eval_f1': 0.8929539295392955, 'eval_precision': 0.8299748110831234, 'eval_recall': 0.966275659824047, 'eval_runtime': 15.0574, 'eval_samples_per_second': 312.404, 'epoch': 1.57}\u001b[0m\n",
      "\u001b[34m{'loss': 0.1001, 'learning_rate': 2.986791648913507e-05, 'epoch': 1.62}\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:17:04.916 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:17:04.916 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:17:04.917 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:DistilBertForSequenceClassification SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.17375051975250244, 'eval_accuracy': 0.9513180272108843, 'eval_f1': 0.9168784029038113, 'eval_precision': 0.907979870596693, 'eval_recall': 0.9259530791788856, 'eval_runtime': 15.3105, 'eval_samples_per_second': 307.24, 'epoch': 1.62}\u001b[0m\n",
      "\u001b[34m{'loss': 0.0857, 'learning_rate': 2.9335321687260335e-05, 'epoch': 1.66}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.1690055876970291, 'eval_accuracy': 0.95046768707483, 'eval_f1': 0.916036036036036, 'eval_precision': 0.9007795889440113, 'eval_recall': 0.9318181818181818, 'eval_runtime': 14.9723, 'eval_samples_per_second': 314.18, 'epoch': 1.66}\u001b[0m\n",
      "\u001b[34m{'loss': 0.1109, 'learning_rate': 2.88027268853856e-05, 'epoch': 1.7}\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:17:39.905 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:17:39.905 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:17:39.906 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:DistilBertForSequenceClassification SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.17395249009132385, 'eval_accuracy': 0.9542942176870748, 'eval_f1': 0.9225783219301406, 'eval_precision': 0.9065817409766455, 'eval_recall': 0.9391495601173021, 'eval_runtime': 15.034, 'eval_samples_per_second': 312.892, 'epoch': 1.7}\u001b[0m\n",
      "\u001b[34m{'loss': 0.1139, 'learning_rate': 2.8270132083510865e-05, 'epoch': 1.74}\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:18:10.948 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:18:10.948 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:18:10.949 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:DistilBertForSequenceClassification SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.18129916489124298, 'eval_accuracy': 0.9519557823129252, 'eval_f1': 0.9193433261955746, 'eval_precision': 0.8956884561891516, 'eval_recall': 0.9442815249266863, 'eval_runtime': 15.3209, 'eval_samples_per_second': 307.031, 'epoch': 1.74}\u001b[0m\n",
      "\u001b[34m{'loss': 0.1184, 'learning_rate': 2.7737537281636132e-05, 'epoch': 1.79}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.17196524143218994, 'eval_accuracy': 0.9528061224489796, 'eval_f1': 0.916036308623298, 'eval_precision': 0.94609375, 'eval_recall': 0.8878299120234604, 'eval_runtime': 15.429, 'eval_samples_per_second': 304.881, 'epoch': 1.79}\u001b[0m\n",
      "\u001b[34m{'loss': 0.0813, 'learning_rate': 2.72049424797614e-05, 'epoch': 1.83}\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:18:46.559 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:18:46.559 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:18:46.560 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:DistilBertForSequenceClassification SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.17368121445178986, 'eval_accuracy': 0.9534438775510204, 'eval_f1': 0.9193370165745856, 'eval_precision': 0.923760177646188, 'eval_recall': 0.9149560117302052, 'eval_runtime': 15.3453, 'eval_samples_per_second': 306.543, 'epoch': 1.83}\u001b[0m\n",
      "\u001b[34m{'loss': 0.0988, 'learning_rate': 2.6672347677886665e-05, 'epoch': 1.87}\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:19:16.663 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:19:16.663 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:19:16.664 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:DistilBertForSequenceClassification SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.1869126707315445, 'eval_accuracy': 0.9534438775510204, 'eval_f1': 0.9207383279044516, 'eval_precision': 0.9092208720514653, 'eval_recall': 0.9325513196480938, 'eval_runtime': 15.1667, 'eval_samples_per_second': 310.152, 'epoch': 1.87}\u001b[0m\n",
      "\u001b[34m{'loss': 0.0852, 'learning_rate': 2.6139752876011932e-05, 'epoch': 1.91}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.18645650148391724, 'eval_accuracy': 0.9553571428571429, 'eval_f1': 0.9231332357247438, 'eval_precision': 0.9217836257309941, 'eval_recall': 0.9244868035190615, 'eval_runtime': 15.014, 'eval_samples_per_second': 313.308, 'epoch': 1.91}\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:19:49.270 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:19:49.270 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:19:49.271 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:DistilBertForSequenceClassification SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m{'loss': 0.1037, 'learning_rate': 2.56071580741372e-05, 'epoch': 1.96}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.18728812038898468, 'eval_accuracy': 0.952593537414966, 'eval_f1': 0.918583424607521, 'eval_precision': 0.9149090909090909, 'eval_recall': 0.9222873900293255, 'eval_runtime': 15.5075, 'eval_samples_per_second': 303.337, 'epoch': 1.96}\u001b[0m\n",
      "\u001b[34m{'loss': 0.0803, 'learning_rate': 2.5074563272262462e-05, 'epoch': 2.0}\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:20:21.812 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:20:21.813 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:20:21.813 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:DistilBertForSequenceClassification SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.1961853802204132, 'eval_accuracy': 0.9523809523809523, 'eval_f1': 0.9190166305133767, 'eval_precision': 0.9065620542082738, 'eval_recall': 0.9318181818181818, 'eval_runtime': 14.9225, 'eval_samples_per_second': 315.229, 'epoch': 2.0}\u001b[0m\n",
      "\u001b[34m{'loss': 0.0362, 'learning_rate': 2.454196847038773e-05, 'epoch': 2.04}\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:20:52.567 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:20:52.567 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:20:52.568 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:DistilBertForSequenceClassification SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.21523593366146088, 'eval_accuracy': 0.9534438775510204, 'eval_f1': 0.9187384044526902, 'eval_precision': 0.9301277235161532, 'eval_recall': 0.907624633431085, 'eval_runtime': 15.2285, 'eval_samples_per_second': 308.894, 'epoch': 2.04}\u001b[0m\n",
      "\u001b[34m{'loss': 0.0308, 'learning_rate': 2.4009373668512995e-05, 'epoch': 2.08}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.2151288539171219, 'eval_accuracy': 0.9528061224489796, 'eval_f1': 0.9187408491947291, 'eval_precision': 0.9173976608187134, 'eval_recall': 0.9200879765395894, 'eval_runtime': 15.2901, 'eval_samples_per_second': 307.65, 'epoch': 2.08}\u001b[0m\n",
      "\u001b[34m{'loss': 0.074, 'learning_rate': 2.3476778866638262e-05, 'epoch': 2.13}\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:21:26.711 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:21:26.711 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:21:26.711 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:DistilBertForSequenceClassification SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.20712295174598694, 'eval_accuracy': 0.9540816326530612, 'eval_f1': 0.9194029850746268, 'eval_precision': 0.9361702127659575, 'eval_recall': 0.9032258064516129, 'eval_runtime': 14.9154, 'eval_samples_per_second': 315.379, 'epoch': 2.13}\u001b[0m\n",
      "\u001b[34m{'loss': 0.0449, 'learning_rate': 2.294418406476353e-05, 'epoch': 2.17}\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:21:57.762 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:21:57.763 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:21:57.763 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:DistilBertForSequenceClassification SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.19981807470321655, 'eval_accuracy': 0.9566326530612245, 'eval_f1': 0.925, 'eval_precision': 0.9277286135693216, 'eval_recall': 0.9222873900293255, 'eval_runtime': 14.9743, 'eval_samples_per_second': 314.139, 'epoch': 2.17}\u001b[0m\n",
      "\u001b[34m{'loss': 0.056, 'learning_rate': 2.2411589262888795e-05, 'epoch': 2.21}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.20544499158859253, 'eval_accuracy': 0.9549319727891157, 'eval_f1': 0.9211895910780669, 'eval_precision': 0.9343891402714932, 'eval_recall': 0.908357771260997, 'eval_runtime': 15.2503, 'eval_samples_per_second': 308.454, 'epoch': 2.21}\u001b[0m\n",
      "\u001b[34m{'loss': 0.0485, 'learning_rate': 2.1878994461014062e-05, 'epoch': 2.25}\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:22:32.561 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:22:32.561 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:22:32.562 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:DistilBertForSequenceClassification SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.19755011796951294, 'eval_accuracy': 0.9532312925170068, 'eval_f1': 0.9208633093525179, 'eval_precision': 0.903954802259887, 'eval_recall': 0.9384164222873901, 'eval_runtime': 15.1535, 'eval_samples_per_second': 310.424, 'epoch': 2.25}\u001b[0m\n",
      "\u001b[34m{'loss': 0.0787, 'learning_rate': 2.1346399659139325e-05, 'epoch': 2.3}\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:23:02.837 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:23:02.838 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:23:02.838 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:DistilBertForSequenceClassification SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.19203440845012665, 'eval_accuracy': 0.9534438775510204, 'eval_f1': 0.9201021524990878, 'eval_precision': 0.915758896151053, 'eval_recall': 0.9244868035190615, 'eval_runtime': 15.4124, 'eval_samples_per_second': 305.208, 'epoch': 2.3}\u001b[0m\n",
      "\u001b[34m{'loss': 0.0552, 'learning_rate': 2.0813804857264592e-05, 'epoch': 2.34}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.20665132999420166, 'eval_accuracy': 0.9553571428571429, 'eval_f1': 0.9227373068432672, 'eval_precision': 0.9261447562776958, 'eval_recall': 0.9193548387096774, 'eval_runtime': 14.9401, 'eval_samples_per_second': 314.858, 'epoch': 2.34}\u001b[0m\n",
      "\u001b[34m{'loss': 0.0372, 'learning_rate': 2.028121005538986e-05, 'epoch': 2.38}\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:23:37.692 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:23:37.693 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:23:37.693 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:DistilBertForSequenceClassification SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.21713383495807648, 'eval_accuracy': 0.9555697278911565, 'eval_f1': 0.9232464193903782, 'eval_precision': 0.9249448123620309, 'eval_recall': 0.9215542521994134, 'eval_runtime': 14.9478, 'eval_samples_per_second': 314.696, 'epoch': 2.38}\u001b[0m\n",
      "\u001b[34m{'loss': 0.0561, 'learning_rate': 1.9748615253515125e-05, 'epoch': 2.42}\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:24:07.582 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:24:07.582 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:24:07.583 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:DistilBertForSequenceClassification SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.23514539003372192, 'eval_accuracy': 0.9517431972789115, 'eval_f1': 0.9182571119913576, 'eval_precision': 0.9023354564755839, 'eval_recall': 0.9347507331378299, 'eval_runtime': 15.2344, 'eval_samples_per_second': 308.775, 'epoch': 2.42}\u001b[0m\n",
      "\u001b[34m{'loss': 0.0287, 'learning_rate': 1.9216020451640392e-05, 'epoch': 2.47}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.2262812703847885, 'eval_accuracy': 0.954719387755102, 'eval_f1': 0.9229656419529837, 'eval_precision': 0.9107780157030693, 'eval_recall': 0.9354838709677419, 'eval_runtime': 15.1149, 'eval_samples_per_second': 311.216, 'epoch': 2.47}\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:24:42.556 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:24:42.556 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:24:42.557 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:DistilBertForSequenceClassification SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m{'loss': 0.0664, 'learning_rate': 1.868342564976566e-05, 'epoch': 2.51}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.2067250907421112, 'eval_accuracy': 0.9545068027210885, 'eval_f1': 0.9217836257309941, 'eval_precision': 0.9190962099125365, 'eval_recall': 0.9244868035190615, 'eval_runtime': 15.1904, 'eval_samples_per_second': 309.67, 'epoch': 2.51}\u001b[0m\n",
      "\u001b[34m{'loss': 0.0382, 'learning_rate': 1.8150830847890925e-05, 'epoch': 2.55}\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:25:12.668 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:25:12.668 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:25:12.668 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:DistilBertForSequenceClassification SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.22607572376728058, 'eval_accuracy': 0.9545068027210885, 'eval_f1': 0.9219547775346462, 'eval_precision': 0.9172714078374455, 'eval_recall': 0.9266862170087976, 'eval_runtime': 15.3974, 'eval_samples_per_second': 305.506, 'epoch': 2.55}\u001b[0m\n",
      "\u001b[34m{'loss': 0.0575, 'learning_rate': 1.7618236046016192e-05, 'epoch': 2.59}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.21537816524505615, 'eval_accuracy': 0.9564200680272109, 'eval_f1': 0.9247706422018348, 'eval_precision': 0.925789860396767, 'eval_recall': 0.9237536656891495, 'eval_runtime': 15.1561, 'eval_samples_per_second': 310.369, 'epoch': 2.59}\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:25:45.892 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:25:45.892 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:25:45.893 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:DistilBertForSequenceClassification SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m{'loss': 0.059, 'learning_rate': 1.708564124414146e-05, 'epoch': 2.64}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.19863295555114746, 'eval_accuracy': 0.9540816326530612, 'eval_f1': 0.9216823785351703, 'eval_precision': 0.9117647058823529, 'eval_recall': 0.9318181818181818, 'eval_runtime': 15.4234, 'eval_samples_per_second': 304.99, 'epoch': 2.64}\u001b[0m\n",
      "\u001b[34m{'loss': 0.0291, 'learning_rate': 1.655304644226672e-05, 'epoch': 2.68}\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:26:18.814 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:26:18.814 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:26:18.815 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:DistilBertForSequenceClassification SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.21454745531082153, 'eval_accuracy': 0.9566326530612245, 'eval_f1': 0.9247787610619469, 'eval_precision': 0.93026706231454, 'eval_recall': 0.9193548387096774, 'eval_runtime': 15.1627, 'eval_samples_per_second': 310.235, 'epoch': 2.68}\u001b[0m\n",
      "\u001b[34m{'loss': 0.0411, 'learning_rate': 1.6020451640391988e-05, 'epoch': 2.72}\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:26:49.743 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:26:49.743 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:26:49.744 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:DistilBertForSequenceClassification SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.22119729220867157, 'eval_accuracy': 0.9564200680272109, 'eval_f1': 0.9255357791500181, 'eval_precision': 0.9172066234701224, 'eval_recall': 0.9340175953079178, 'eval_runtime': 15.2602, 'eval_samples_per_second': 308.253, 'epoch': 2.72}\u001b[0m\n",
      "\u001b[34m{'loss': 0.0359, 'learning_rate': 1.5487856838517255e-05, 'epoch': 2.76}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.2242191880941391, 'eval_accuracy': 0.9538690476190477, 'eval_f1': 0.9217454020915975, 'eval_precision': 0.9070262597586941, 'eval_recall': 0.9369501466275659, 'eval_runtime': 15.1955, 'eval_samples_per_second': 309.564, 'epoch': 2.76}\u001b[0m\n",
      "\u001b[34m{'loss': 0.0535, 'learning_rate': 1.4955262036642523e-05, 'epoch': 2.81}\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:27:24.069 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:27:24.069 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:27:24.070 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:DistilBertForSequenceClassification SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.21895043551921844, 'eval_accuracy': 0.954719387755102, 'eval_f1': 0.9220065909923106, 'eval_precision': 0.9209948792977323, 'eval_recall': 0.9230205278592375, 'eval_runtime': 15.0971, 'eval_samples_per_second': 311.583, 'epoch': 2.81}\u001b[0m\n",
      "\u001b[34m{'loss': 0.0704, 'learning_rate': 1.442266723476779e-05, 'epoch': 2.85}\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:27:54.332 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:27:54.332 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:27:54.333 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:DistilBertForSequenceClassification SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.19882525503635406, 'eval_accuracy': 0.9570578231292517, 'eval_f1': 0.9263848396501458, 'eval_precision': 0.9210144927536232, 'eval_recall': 0.9318181818181818, 'eval_runtime': 15.0505, 'eval_samples_per_second': 312.547, 'epoch': 2.85}\u001b[0m\n",
      "\u001b[34m{'loss': 0.0401, 'learning_rate': 1.3890072432893057e-05, 'epoch': 2.89}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.2053094208240509, 'eval_accuracy': 0.9581207482993197, 'eval_f1': 0.9269016697588126, 'eval_precision': 0.9383921863260706, 'eval_recall': 0.9156891495601173, 'eval_runtime': 15.2991, 'eval_samples_per_second': 307.47, 'epoch': 2.89}\u001b[0m\n",
      "\u001b[34m{'loss': 0.0372, 'learning_rate': 1.3357477631018323e-05, 'epoch': 2.93}\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:28:29.517 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:28:29.518 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:28:29.518 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:DistilBertForSequenceClassification SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.2158953845500946, 'eval_accuracy': 0.9534438775510204, 'eval_f1': 0.9218694256154121, 'eval_precision': 0.8978457261987491, 'eval_recall': 0.9472140762463344, 'eval_runtime': 15.0973, 'eval_samples_per_second': 311.579, 'epoch': 2.93}\u001b[0m\n",
      "\u001b[34m{'loss': 0.0303, 'learning_rate': 1.2824882829143588e-05, 'epoch': 2.98}\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:28:59.643 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:28:59.643 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:28:59.644 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:DistilBertForSequenceClassification SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.21090498566627502, 'eval_accuracy': 0.9589710884353742, 'eval_f1': 0.9297415362213324, 'eval_precision': 0.923355025307303, 'eval_recall': 0.9362170087976539, 'eval_runtime': 15.2006, 'eval_samples_per_second': 309.462, 'epoch': 2.98}\u001b[0m\n",
      "\u001b[34m{'loss': 0.0348, 'learning_rate': 1.2292288027268855e-05, 'epoch': 3.02}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.2318396270275116, 'eval_accuracy': 0.952593537414966, 'eval_f1': 0.9204423831608992, 'eval_precision': 0.896455872133426, 'eval_recall': 0.9457478005865103, 'eval_runtime': 15.076, 'eval_samples_per_second': 312.018, 'epoch': 3.02}\u001b[0m\n",
      "\u001b[34m{'loss': 0.045, 'learning_rate': 1.1759693225394122e-05, 'epoch': 3.06}\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:29:35.921 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:29:35.921 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:29:35.922 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:DistilBertForSequenceClassification SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.20919761061668396, 'eval_accuracy': 0.9579081632653061, 'eval_f1': 0.9278425655976676, 'eval_precision': 0.922463768115942, 'eval_recall': 0.9332844574780058, 'eval_runtime': 15.3429, 'eval_samples_per_second': 306.592, 'epoch': 3.06}\u001b[0m\n",
      "\u001b[34m{'loss': 0.02, 'learning_rate': 1.1227098423519387e-05, 'epoch': 3.1}\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:30:05.887 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:30:05.887 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:30:05.888 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:DistilBertForSequenceClassification SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.212579607963562, 'eval_accuracy': 0.9579081632653061, 'eval_f1': 0.9278951201747998, 'eval_precision': 0.9218523878437048, 'eval_recall': 0.9340175953079178, 'eval_runtime': 15.3019, 'eval_samples_per_second': 307.412, 'epoch': 3.1}\u001b[0m\n",
      "\u001b[34m{'loss': 0.0018, 'learning_rate': 1.0694503621644653e-05, 'epoch': 3.15}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.22721590101718903, 'eval_accuracy': 0.9568452380952381, 'eval_f1': 0.9262086513994909, 'eval_precision': 0.9185291997116077, 'eval_recall': 0.9340175953079178, 'eval_runtime': 15.1748, 'eval_samples_per_second': 309.987, 'epoch': 3.15}\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:30:40.775 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:30:40.775 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:30:40.776 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:DistilBertForSequenceClassification SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m{'loss': 0.0212, 'learning_rate': 1.016190881976992e-05, 'epoch': 3.19}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.23212848603725433, 'eval_accuracy': 0.9570578231292517, 'eval_f1': 0.9257898603967671, 'eval_precision': 0.9278350515463918, 'eval_recall': 0.9237536656891495, 'eval_runtime': 15.0583, 'eval_samples_per_second': 312.385, 'epoch': 3.19}\u001b[0m\n",
      "\u001b[34m{'loss': 0.0025, 'learning_rate': 9.629314017895187e-06, 'epoch': 3.23}\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:31:11.060 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:31:11.060 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:31:11.061 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:DistilBertForSequenceClassification SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.23865193128585815, 'eval_accuracy': 0.9572704081632653, 'eval_f1': 0.9267759562841529, 'eval_precision': 0.9210716871832005, 'eval_recall': 0.9325513196480938, 'eval_runtime': 15.2502, 'eval_samples_per_second': 308.454, 'epoch': 3.23}\u001b[0m\n",
      "\u001b[34m{'loss': 0.0233, 'learning_rate': 9.096719216020452e-06, 'epoch': 3.27}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.24012109637260437, 'eval_accuracy': 0.9583333333333334, 'eval_f1': 0.9286234522942461, 'eval_precision': 0.9225759768451519, 'eval_recall': 0.9347507331378299, 'eval_runtime': 15.1925, 'eval_samples_per_second': 309.627, 'epoch': 3.27}\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:31:42.861 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:31:42.861 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:31:42.862 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:DistilBertForSequenceClassification SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m{'loss': 0.0109, 'learning_rate': 8.564124414145718e-06, 'epoch': 3.32}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.24493244290351868, 'eval_accuracy': 0.9585459183673469, 'eval_f1': 0.9279112754158965, 'eval_precision': 0.935868754660701, 'eval_recall': 0.9200879765395894, 'eval_runtime': 15.1201, 'eval_samples_per_second': 311.11, 'epoch': 3.32}\u001b[0m\n",
      "\u001b[34m{'loss': 0.013, 'learning_rate': 8.031529612270985e-06, 'epoch': 3.36}\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:32:15.904 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:32:15.904 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:32:15.905 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:DistilBertForSequenceClassification SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.2501456141471863, 'eval_accuracy': 0.9572704081632653, 'eval_f1': 0.926508226691042, 'eval_precision': 0.9241429613420861, 'eval_recall': 0.9288856304985337, 'eval_runtime': 15.1625, 'eval_samples_per_second': 310.239, 'epoch': 3.36}\u001b[0m\n",
      "\u001b[34m{'loss': 0.0248, 'learning_rate': 7.498934810396251e-06, 'epoch': 3.4}\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:32:46.439 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:32:46.439 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:32:46.440 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:DistilBertForSequenceClassification SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.254604697227478, 'eval_accuracy': 0.9555697278911565, 'eval_f1': 0.9243027888446215, 'eval_precision': 0.9133858267716536, 'eval_recall': 0.9354838709677419, 'eval_runtime': 15.0877, 'eval_samples_per_second': 311.777, 'epoch': 3.4}\u001b[0m\n",
      "\u001b[34m{'loss': 0.0052, 'learning_rate': 6.966340008521517e-06, 'epoch': 3.44}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.2546684145927429, 'eval_accuracy': 0.9570578231292517, 'eval_f1': 0.9267053701015965, 'eval_precision': 0.9173850574712644, 'eval_recall': 0.9362170087976539, 'eval_runtime': 15.3215, 'eval_samples_per_second': 307.02, 'epoch': 3.44}\u001b[0m\n",
      "\u001b[34m{'loss': 0.0112, 'learning_rate': 6.433745206646784e-06, 'epoch': 3.49}\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:33:25.748 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:33:25.749 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:33:25.749 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:DistilBertForSequenceClassification SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.25246891379356384, 'eval_accuracy': 0.9572704081632653, 'eval_f1': 0.9266155531215772, 'eval_precision': 0.9229090909090909, 'eval_recall': 0.9303519061583577, 'eval_runtime': 18.8657, 'eval_samples_per_second': 249.342, 'epoch': 3.49}\u001b[0m\n",
      "\u001b[34m{'loss': 0.0196, 'learning_rate': 5.90115040477205e-06, 'epoch': 3.53}\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:33:55.935 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:33:55.936 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:33:55.936 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:DistilBertForSequenceClassification SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.2538944184780121, 'eval_accuracy': 0.9574829931972789, 'eval_f1': 0.9277978339350181, 'eval_precision': 0.9139402560455192, 'eval_recall': 0.9420821114369502, 'eval_runtime': 15.0475, 'eval_samples_per_second': 312.61, 'epoch': 3.53}\u001b[0m\n",
      "\u001b[34m{'loss': 0.0052, 'learning_rate': 5.3685556028973165e-06, 'epoch': 3.57}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.2595914602279663, 'eval_accuracy': 0.9566326530612245, 'eval_f1': 0.9266187050359712, 'eval_precision': 0.9096045197740112, 'eval_recall': 0.9442815249266863, 'eval_runtime': 15.3172, 'eval_samples_per_second': 307.106, 'epoch': 3.57}\u001b[0m\n",
      "\u001b[34m{'loss': 0.0106, 'learning_rate': 4.835960801022582e-06, 'epoch': 3.61}\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:34:31.076 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:34:31.076 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:34:31.077 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:DistilBertForSequenceClassification SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.2586190104484558, 'eval_accuracy': 0.9566326530612245, 'eval_f1': 0.9263537906137185, 'eval_precision': 0.9125177809388336, 'eval_recall': 0.9406158357771262, 'eval_runtime': 15.1584, 'eval_samples_per_second': 310.323, 'epoch': 3.61}\u001b[0m\n",
      "\u001b[34m{'loss': 0.0166, 'learning_rate': 4.303365999147849e-06, 'epoch': 3.66}\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:35:01.035 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:35:01.035 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:35:01.036 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:DistilBertForSequenceClassification SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.25970014929771423, 'eval_accuracy': 0.9568452380952381, 'eval_f1': 0.9265291349981903, 'eval_precision': 0.9149392423159399, 'eval_recall': 0.9384164222873901, 'eval_runtime': 15.4132, 'eval_samples_per_second': 305.193, 'epoch': 3.66}\u001b[0m\n",
      "\u001b[34m{'loss': 0.0155, 'learning_rate': 3.7707711972731147e-06, 'epoch': 3.7}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.262484610080719, 'eval_accuracy': 0.9574829931972789, 'eval_f1': 0.9273783587509078, 'eval_precision': 0.918705035971223, 'eval_recall': 0.9362170087976539, 'eval_runtime': 15.2031, 'eval_samples_per_second': 309.411, 'epoch': 3.7}\u001b[0m\n",
      "\u001b[34m{'loss': 0.0156, 'learning_rate': 3.238176395398381e-06, 'epoch': 3.74}\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:35:36.285 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:35:36.285 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:35:36.285 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:DistilBertForSequenceClassification SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.2607070505619049, 'eval_accuracy': 0.9574829931972789, 'eval_f1': 0.927007299270073, 'eval_precision': 0.9229651162790697, 'eval_recall': 0.9310850439882697, 'eval_runtime': 15.1404, 'eval_samples_per_second': 310.692, 'epoch': 3.74}\u001b[0m\n",
      "\u001b[34m{'loss': 0.0315, 'learning_rate': 2.7055815935236472e-06, 'epoch': 3.78}\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:36:06.238 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:36:06.238 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:36:06.238 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:DistilBertForSequenceClassification SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.25841137766838074, 'eval_accuracy': 0.9576955782312925, 'eval_f1': 0.9273457466228551, 'eval_precision': 0.9236363636363636, 'eval_recall': 0.9310850439882697, 'eval_runtime': 15.2868, 'eval_samples_per_second': 307.716, 'epoch': 3.78}\u001b[0m\n",
      "\u001b[34m{'loss': 0.0307, 'learning_rate': 2.1729867916489135e-06, 'epoch': 3.83}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.25611016154289246, 'eval_accuracy': 0.9581207482993197, 'eval_f1': 0.9279707495429615, 'eval_precision': 0.925601750547046, 'eval_recall': 0.9303519061583577, 'eval_runtime': 15.2078, 'eval_samples_per_second': 309.316, 'epoch': 3.83}\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:36:41.581 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:36:41.581 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:36:41.582 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:DistilBertForSequenceClassification SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m{'loss': 0.0254, 'learning_rate': 1.6403919897741797e-06, 'epoch': 3.87}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.2563239336013794, 'eval_accuracy': 0.9576955782312925, 'eval_f1': 0.927292656192912, 'eval_precision': 0.9242534595775673, 'eval_recall': 0.9303519061583577, 'eval_runtime': 15.1472, 'eval_samples_per_second': 310.552, 'epoch': 3.87}\u001b[0m\n",
      "\u001b[34m{'loss': 0.0097, 'learning_rate': 1.1077971878994461e-06, 'epoch': 3.91}\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:37:12.438 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:37:12.438 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:37:12.439 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:DistilBertForSequenceClassification SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.25676703453063965, 'eval_accuracy': 0.9579081632653061, 'eval_f1': 0.9275786393562546, 'eval_precision': 0.9255474452554745, 'eval_recall': 0.9296187683284457, 'eval_runtime': 15.3718, 'eval_samples_per_second': 306.014, 'epoch': 3.91}\u001b[0m\n",
      "\u001b[34m{'loss': 0.0162, 'learning_rate': 5.752023860247124e-07, 'epoch': 3.95}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.2562606632709503, 'eval_accuracy': 0.9583333333333334, 'eval_f1': 0.9283101682516459, 'eval_precision': 0.9262773722627737, 'eval_recall': 0.9303519061583577, 'eval_runtime': 15.0797, 'eval_samples_per_second': 311.942, 'epoch': 3.95}\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:37:43.913 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:37:43.914 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:37:43.914 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:DistilBertForSequenceClassification SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m{'loss': 0.0193, 'learning_rate': 4.26075841499787e-08, 'epoch': 4.0}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.2564370632171631, 'eval_accuracy': 0.9583333333333334, 'eval_f1': 0.9283625730994153, 'eval_precision': 0.9256559766763849, 'eval_recall': 0.9310850439882697, 'eval_runtime': 15.3421, 'eval_samples_per_second': 306.608, 'epoch': 4.0}\u001b[0m\n",
      "\u001b[34m{'train_runtime': 2106.6375, 'train_samples_per_second': 2.233, 'epoch': 4.0}\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:38:13.248 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:38:13.248 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:38:13.249 algo-1:52 WARNING hook.py:898] var is not Tensor or list or tuple of Tensors, module_name:DistilBertForSequenceClassification SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m***** Eval results *****\u001b[0m\n",
      "\u001b[34m[2021-03-08 15:38:21.068 algo-1:52 INFO utils.py:25] The end of training job file will not be written for jobs running under SageMaker.\u001b[0m\n",
      "\u001b[34m2021-03-08 15:38:21,555 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2021-03-08 15:38:59 Uploading - Uploading generated training model\n",
      "2021-03-08 15:41:01 Completed - Training job completed\n",
      "Training seconds: 2438\n",
      "Billable seconds: 2438\n"
     ]
    }
   ],
   "source": [
    "bert_estimator.fit({'training': inputs_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "single-observation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-1-649363377072/pytorch-training-2021-03-08-14-56-50-390/output/model.tar.gz'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_estimator.model_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mighty-graduate",
   "metadata": {},
   "source": [
    "# SageMaker Endpoint Deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "spanish-blood",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.pytorch.model import PyTorchModel\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "from sagemaker.serializers import JSONSerializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "banned-speaker",
   "metadata": {},
   "outputs": [],
   "source": [
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "unavailable-penalty",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = \"s3://sagemaker-us-east-1-649363377072/pytorch-training-2021-03-08-14-56-50-390/output/model.tar.gz\"\n",
    "src_dir = 'src'\n",
    "\n",
    "pytorch_model = PyTorchModel(model_data=model_data,\n",
    "                             role=role,\n",
    "                             framework_version=\"1.4.0\",\n",
    "                             source_dir=src_dir,\n",
    "                             py_version=\"py3\",\n",
    "                             entry_point=\"hf_train_deploy.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "extra-world",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------!"
     ]
    }
   ],
   "source": [
    "predictor = pytorch_model.deploy(initial_instance_count=1, \n",
    "                                 instance_type=\"ml.m5.large\", \n",
    "                                 endpoint_name='HF-BERT-AE-model',\n",
    "                                 serializer=JSONSerializer(),\n",
    "                                 deserializer=JSONDeserializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reported-postage",
   "metadata": {},
   "source": [
    "# Inference: invoke SageMaker Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "particular-manner",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "pediatric-chase",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = 'HF-BERT-AE-model'\n",
    "runtime= boto3.client('runtime.sagemaker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "sudden-pendant",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'This entity is probably related to a combination of high doses of corticosteroids, vecuronium administration and metabolic abnormalities associated with respiratory failure.'\n",
    "\n",
    "\n",
    "response = runtime.invoke_endpoint(EndpointName=endpoint_name,\n",
    "                                   ContentType='application/json',\n",
    "                                   Body=json.dumps(query))\n",
    "prob = eval(response['Body'].read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "technological-latitude",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.6\n",
    "\n",
    "prd_prob = prob[1]\n",
    "pred_label = \"Adverse Event (AE)\" if prd_prob >= threshold else \"non ae\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "knowing-vitamin",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Adverse Event (AE)', 0.9993265867233276)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_label, prd_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hidden-subscription",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}